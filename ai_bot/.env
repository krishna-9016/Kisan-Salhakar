# --- Data and Model Paths ---
KNOWLEDGE_BASE_DIR="data/knowledge_base"
VECTOR_STORE_DIR="vector_store"
FINETUNING_DATASET_PATH="data/finetuning_dataset.json"


# --- Model Configuration ---
# Embedding model for RAG
EMBEDDING_MODEL_NAME="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"


# Base LLM for fine-tuning and inference. 
# IMPORTANT: Choose a Causal (decoder-only) model. distilgpt2 is a small example.
# For better Punjabi performance, consider models like 'ai4bharat/indic-gpt' or other multilingual GPTs.

BASE_MODEL_NAME=C:/huggingface/models/BharatGPT-3B-Indic

# Name for the saved LoRA adapter
ADAPTER_MODEL_NAME="punjabi-farmer-advisor-v1"


GOOGLE_API_KEY = "AIzaSyDcsWRLd06-B2mQ_F3XuRM0sjXZa7xNu5E"